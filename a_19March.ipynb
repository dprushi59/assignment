{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ed5a0ec3-2601-4ec0-856a-a0bf1425f39d",
   "metadata": {},
   "source": [
    "#1\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used in machine learning to scale the features of a dataset to a specific range, typically between 0 and 1. This technique is employed to ensure that all features have a similar scale, which can improve the performance of certain machine learning algorithms, particularly those that are sensitive to the scale of the input features. \n",
    "\n",
    "The formula for Min-Max scaling is as follows for each feature:\n",
    "\n",
    "\\[ X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\]\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the original feature value.\n",
    "- \\(X_{scaled}\\) is the scaled feature value.\n",
    "- \\(X_{min}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{max}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Let's say you have a dataset of housing prices with two features: \"Area\" (measured in square feet) and \"Number of Bedrooms\". The \"Area\" feature has values ranging from 500 to 3000 square feet, while the \"Number of Bedrooms\" feature ranges from 1 to 5 bedrooms.\n",
    "\n",
    "Original data (a few samples):\n",
    "| Area (sq. ft.) | Bedrooms |\n",
    "|----------------|----------|\n",
    "| 1000           | 2        |\n",
    "| 1500           | 3        |\n",
    "| 2000           | 4        |\n",
    "| 2500           | 5        |\n",
    "\n",
    "To apply Min-Max scaling, you would calculate the minimum and maximum values for each feature:\n",
    "\n",
    "- Minimum Area: 500\n",
    "- Maximum Area: 3000\n",
    "- Minimum Bedrooms: 1\n",
    "- Maximum Bedrooms: 5\n",
    "\n",
    "Then, you would use the formula to scale the features:\n",
    "\n",
    "For the \"Area\" feature:\n",
    "\\[ X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\]\n",
    "For example, for an \"Area\" of 1500 sq. ft.:\n",
    "\\[ X_{scaled} = \\frac{1500 - 500}{3000 - 500} = 0.375 \\]\n",
    "\n",
    "For the \"Number of Bedrooms\" feature, the formula is applied similarly.\n",
    "\n",
    "After scaling, the data might look like this:\n",
    "\n",
    "| Area (scaled) | Bedrooms (scaled) |\n",
    "|---------------|-------------------|\n",
    "| 0.375         | 0.25              |\n",
    "| 0.5625        | 0.5               |\n",
    "| 0.75          | 0.75              |\n",
    "| 0.9375        | 1.0               |\n",
    "\n",
    "Now, both features are within the range of 0 to 1, and they have a similar scale, making them suitable for machine learning algorithms that require scaled features, such as neural networks, support vector machines, and k-nearest neighbors.\n",
    "\n",
    "Remember that Min-Max scaling doesn't guarantee better performance for all algorithms. Some algorithms are robust to feature scaling, and for others, alternative scaling methods like Z-score normalization might be more appropriate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "266b53a3-a309-41b3-8f2e-27d01b9c2cd8",
   "metadata": {},
   "source": [
    "#2\n",
    "The Unit Vector technique, also known as \"Normalization\" or \"L2 normalization,\" is a feature scaling method that scales the values of a feature to have a unit vector length (i.e., a Euclidean norm of 1). This technique is particularly useful when the magnitude of the feature values is important, but you still want to ensure that all features have a comparable scale.\n",
    "\n",
    "Unlike Min-Max scaling, which scales features to a specific range (e.g., between 0 and 1), Unit Vector scaling doesn't necessarily constrain the values within a specific range. Instead, it focuses on the direction of the data points in the feature space, ensuring that they all lie on the surface of a unit sphere.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "\\[ X_{normalized} = \\frac{X}{\\|X\\|} \\]\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the original feature vector.\n",
    "- \\(X_{normalized}\\) is the normalized feature vector.\n",
    "- \\(\\|X\\|\\) is the Euclidean norm of the feature vector, calculated as \\(\\sqrt{X_1^2 + X_2^2 + \\ldots + X_n^2}\\), where \\(X_1, X_2, \\ldots, X_n\\) are the individual components of the feature vector.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset of customer transactions with two features: \"Amount Spent\" (in dollars) and \"Number of Items Purchased\". The \"Amount Spent\" feature has values ranging from 50 to 1000 dollars, while the \"Number of Items Purchased\" feature ranges from 1 to 20 items.\n",
    "\n",
    "Original data (a few samples):\n",
    "| Amount Spent ($) | Items Purchased |\n",
    "|------------------|----------------|\n",
    "| 200              | 5              |\n",
    "| 500              | 10             |\n",
    "| 1000             | 20             |\n",
    "| 100              | 2              |\n",
    "\n",
    "To apply Unit Vector scaling, you first calculate the Euclidean norm for each data point:\n",
    "\n",
    "For the data point [200, 5]:\n",
    "\\[ \\|X\\| = \\sqrt{200^2 + 5^2} = \\sqrt{40000 + 25} \\approx 200.624 \\]\n",
    "\n",
    "Then, you normalize the feature vector using the formula:\n",
    "\\[ X_{normalized} = \\frac{X}{\\|X\\|} \\]\n",
    "For example, for the data point [200, 5]:\n",
    "\\[ X_{normalized} = \\left[\\frac{200}{200.624}, \\frac{5}{200.624}\\right] \\approx [0.998, 0.025] \\]\n",
    "\n",
    "After normalization, the data might look like this:\n",
    "\n",
    "| Amount Spent (normalized) | Items Purchased (normalized) |\n",
    "|--------------------------|-------------------------------|\n",
    "| [0.998, 0.025]           | [0.996, 0.090]                |\n",
    "| [0.995, 0.099]           | [0.995, 0.099]                |\n",
    "| [0.997, 0.075]           | [0.995, 0.099]                |\n",
    "| [0.996, 0.081]           | [0.995, 0.099]                |\n",
    "\n",
    "In this example, the magnitudes of the feature vectors have been scaled to a unit vector length, while preserving the direction of the original data points. This is particularly useful when you want to emphasize the direction of the data points rather than their magnitudes, and it can be helpful in various machine learning scenarios, such as text classification using word embeddings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b96f9a31-1350-4636-a13d-269884c272fa",
   "metadata": {},
   "source": [
    "#3\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. Its main goal is to transform a dataset into a new coordinate system, where the data's variability is maximized along a few orthogonal axes called principal components. These principal components are linear combinations of the original features and are ordered in terms of the amount of variance they capture.\n",
    "\n",
    "PCA is commonly used to reduce the number of features in a dataset while retaining as much relevant information as possible. This is especially valuable when dealing with high-dimensional data, as it can help mitigate issues like the curse of dimensionality, reduce noise, and improve the efficiency of machine learning algorithms.\n",
    "\n",
    "Here's a high-level overview of how PCA works:\n",
    "\n",
    "1. **Compute the Covariance Matrix:** Calculate the covariance matrix of the original dataset. This matrix provides information about the relationships between different features.\n",
    "\n",
    "2. **Calculate Eigenvectors and Eigenvalues:** Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) along which the data has the maximum variance, while eigenvalues indicate the amount of variance along each eigenvector.\n",
    "\n",
    "3. **Sort Eigenvectors by Eigenvalues:** Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with the highest eigenvalues represent the principal components that capture the most variance in the data.\n",
    "\n",
    "4. **Select Principal Components:** Choose a subset of the top principal components based on the amount of variance you want to retain. These components will form the new coordinate system for your data.\n",
    "\n",
    "5. **Transform Data:** Transform the original data into the new coordinate system defined by the selected principal components. This involves projecting each data point onto the principal components.\n",
    "\n",
    "Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset of customer shopping preferences with five features: \"Time Spent Shopping,\" \"Amount Spent,\" \"Number of Items Purchased,\" \"Age,\" and \"Income.\" You want to reduce the dimensionality of this dataset while preserving as much variance as possible.\n",
    "\n",
    "Original data (a few samples):\n",
    "| Time Spent | Amount Spent | Items Purchased | Age | Income   |\n",
    "|------------|--------------|-----------------|-----|----------|\n",
    "| 45         | 100          | 5               | 30  | 50000    |\n",
    "| 60         | 150          | 7               | 25  | 60000    |\n",
    "| 30         | 80           | 3               | 40  | 55000    |\n",
    "| 90         | 200          | 10              | 35  | 75000    |\n",
    "\n",
    "To apply PCA for dimensionality reduction:\n",
    "\n",
    "1. Compute the covariance matrix based on the original data.\n",
    "2. Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "3. Sort the eigenvectors by eigenvalues and select the top \\(k\\) eigenvectors to form the new coordinate system (where \\(k\\) is the desired reduced dimensionality).\n",
    "4. Transform the original data using the selected eigenvectors.\n",
    "\n",
    "Let's say you decide to reduce the dimensionality to 2 principal components (from 5 original features). After performing PCA, you might find that the first two principal components capture 90% of the variance in the data.\n",
    "\n",
    "Transformed data (using the first two principal components):\n",
    "| Principal Component 1 | Principal Component 2 |\n",
    "|-----------------------|-----------------------|\n",
    "| -0.656                | -0.060                |\n",
    "| 0.303                 | -0.039                |\n",
    "| -0.700                | 0.078                 |\n",
    "| 1.052                 | 0.021                 |\n",
    "\n",
    "In this transformed dataset, you have reduced the dimensionality from 5 to 2 while retaining a significant portion of the variance. These two new features (principal components) are linear combinations of the original features and can be used for further analysis or machine learning tasks.\n",
    "\n",
    "Remember that PCA is most effective when the data has some inherent structure or patterns. It may not be as useful when the data doesn't exhibit clear variance along specific axes. Additionally, interpreting the principal components can be challenging, especially if the original features don't have clear semantic meanings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "834ad70a-5fce-481e-84a3-62b82cd8e42c",
   "metadata": {},
   "source": [
    "#4\n",
    "Principal Component Analysis (PCA) is a technique used in both dimensionality reduction and feature extraction. It's a statistical procedure that transforms a high-dimensional dataset into a lower-dimensional representation while preserving as much of the original variability as possible. PCA achieves this by identifying and selecting the principal components, which are linear combinations of the original features that capture the most significant patterns in the data.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA can be used as a method for extracting new features from the existing ones. It does this by finding a set of orthogonal axes (principal components) along which the data varies the most. These principal components can then be used as the new features, potentially reducing the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "**Example: Face Recognition**\n",
    "\n",
    "Suppose you have a dataset of grayscale images of faces, each represented as a vector of pixel intensities. Each image is 100x100 pixels, resulting in a high-dimensional feature space of 10,000 dimensions (100 * 100 = 10,000). However, many of these dimensions might be redundant or noisy, and using all of them can lead to computational inefficiencies and overfitting.\n",
    "\n",
    "You want to reduce the dimensionality of the dataset while retaining the essential information needed for face recognition. This is where PCA comes into play:\n",
    "\n",
    "1. **Data Preparation**: Convert each image into a flattened vector, resulting in a matrix where each row corresponds to an image and each column corresponds to a pixel intensity value.\n",
    "\n",
    "2. **Mean Subtraction**: Subtract the mean of each feature (pixel) across all samples to center the data.\n",
    "\n",
    "3. **Covariance Matrix Calculation**: Calculate the covariance matrix of the centered data. This matrix represents how the different features are correlated with each other.\n",
    "\n",
    "4. **Eigenvector Calculation**: Calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors (principal components) represent the directions along which the data varies the most.\n",
    "\n",
    "5. **Feature Extraction**: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues capture the most significant variations in the data. You can choose the top k eigenvectors to use as the new features.\n",
    "\n",
    "6. **Projecting Data**: Transform the original data into the lower-dimensional space spanned by the selected eigenvectors. This is achieved by projecting each data point onto the subspace defined by the top k eigenvectors.\n",
    "\n",
    "By using the top k eigenvectors as the new features, you've effectively extracted a reduced set of dimensions that capture the most important variations in the data. These new dimensions can be used for tasks like face recognition while reducing computational complexity and potentially enhancing the model's generalization.\n",
    "\n",
    "In this example, PCA served as a feature extraction technique by selecting the most important dimensions (principal components) from the original high-dimensional dataset, allowing for efficient and effective face recognition without sacrificing critical information."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b499b57-1dfa-4ba3-892c-d2e712855121",
   "metadata": {},
   "source": [
    "#5\n",
    "Min-Max scaling is a common data preprocessing technique used to transform numerical features in a dataset to a specific range, typically between 0 and 1. This is done to ensure that all the features contribute equally to the analysis and modeling process, especially when the features have different scales or units. In the context of building a recommendation system for a food delivery service, where features like price, rating, and delivery time are important, Min-Max scaling can help normalize these features to a consistent range.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess the data for your recommendation system project:\n",
    "\n",
    "1. **Understand the Data**: First, ensure you have a clear understanding of the data and the specific features you're dealing with. In your case, you have features like price, rating, and delivery time.\n",
    "\n",
    "2. **Feature Selection**: Decide which features need to be scaled. Not all features might require scaling; for instance, if you have categorical features like cuisine type, scaling wouldn't be necessary.\n",
    "\n",
    "3. **Min-Max Scaling Formula**:\n",
    "   - The formula to perform Min-Max scaling on a feature x is:\n",
    "   \n",
    "     scaled_x = (x - min(x)) / (max(x) - min(x))\n",
    "     \n",
    "     where min(x) is the minimum value of the feature x, and max(x) is the maximum value of the feature x in the dataset.\n",
    "\n",
    "4. **Apply Min-Max Scaling**: Calculate the minimum and maximum values for each feature (price, rating, delivery time) in your dataset. Then, apply the Min-Max scaling formula to each data point for each feature.\n",
    "\n",
    "   For example, let's say you have the following raw values for each feature:\n",
    "   - Price: [5, 10, 15, 20, 25]\n",
    "   - Rating: [3.2, 4.5, 2.8, 4.0, 3.9]\n",
    "   - Delivery Time (minutes): [30, 40, 20, 60, 45]\n",
    "   \n",
    "   To scale these features using Min-Max scaling, you would:\n",
    "   - Price: Apply the formula to each value in the Price feature using the minimum (5) and maximum (25) values.\n",
    "   - Rating: Apply the formula to each value in the Rating feature using the minimum (2.8) and maximum (4.5) values.\n",
    "   - Delivery Time: Apply the formula to each value in the Delivery Time feature using the minimum (20) and maximum (60) values.\n",
    "\n",
    "5. **Interpretation**: After applying Min-Max scaling, all the features will be transformed to a range between 0 and 1. This ensures that features with different scales, like price and delivery time, are on a comparable scale. The scaled features are now ready to be used for your recommendation system.\n",
    "\n",
    "Min-Max scaling is a straightforward technique to normalize your numerical features within a specific range. It helps prevent certain features from dominating the recommendation process due to their larger scales, leading to a more balanced and accurate recommendation system."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fb161ee-c7b2-4d34-88d9-44f0fb214541",
   "metadata": {},
   "source": [
    "#6\n",
    "In the context of building a model to predict stock prices, where the dataset contains numerous features including company financial data and market trends, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset while retaining the most relevant information. By doing so, you can mitigate the curse of dimensionality, improve computational efficiency, and potentially enhance the generalization ability of your predictive model.\n",
    "\n",
    "Here's how you would use PCA to reduce the dimensionality of the stock price prediction dataset:\n",
    "\n",
    "1. **Data Preparation**: Gather and organize the dataset containing features related to company financial data and market trends. This dataset will likely have a large number of features (dimensions).\n",
    "\n",
    "2. **Data Standardization**: Before applying PCA, it's important to standardize or normalize the features to have a mean of 0 and a standard deviation of 1. This step ensures that features with different scales do not disproportionately influence the PCA results.\n",
    "\n",
    "3. **Covariance Matrix Calculation**: Calculate the covariance matrix of the standardized dataset. The covariance matrix represents the relationships and correlations between the features.\n",
    "\n",
    "4. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. This process yields the eigenvectors and eigenvalues of the matrix. Eigenvectors represent the directions of maximum variance (principal components), and eigenvalues indicate the variance explained by each eigenvector.\n",
    "\n",
    "5. **Selecting Principal Components**: Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with the highest eigenvalues capture the most significant variations in the data. You need to decide how many principal components to retain, which will determine the new reduced dimensionality of the dataset. You can use various methods, such as the cumulative explained variance or a certain threshold of variance explained.\n",
    "\n",
    "6. **Projection**: Transform the original data into the lower-dimensional space spanned by the selected principal components. This is done by projecting each data point onto the subspace defined by the top k principal components.\n",
    "\n",
    "7. **Modeling**: Use the reduced-dimension dataset as input for your stock price prediction model. You'll be working with a smaller set of features that retain the most relevant information while reducing noise and redundancy.\n",
    "\n",
    "8. **Inverse Transformation (Optional)**: If needed, you can apply the inverse transformation to the predictions obtained from the reduced-dimension dataset to map them back to the original feature space.\n",
    "\n",
    "By applying PCA in this manner, you're effectively reducing the dimensionality of your dataset while preserving the most important information. This can help your predictive model handle the complexity of the data more efficiently and potentially improve its performance. Keep in mind that while PCA can be a powerful technique for dimensionality reduction, it's important to strike a balance between reducing dimensionality and maintaining the interpretability of the features in the context of stock price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9a1808-5e73-45dd-b5ae-a4d4810feba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.47368421052631576,\n",
       " -0.2631578947368421,\n",
       " 0.0,\n",
       " 0.2631578947368421,\n",
       " 0.5263157894736842]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "l = [1,5,10,15,20]\n",
    "maximum = 20\n",
    "minimum = 1\n",
    "scaledmM_l = []\n",
    "for i in l:\n",
    "    x = ( i- 10) / (maximum - minimum)  #for the range of (-1,1)\n",
    "    scaledmM_l.append(x)\n",
    "scaledmM_l    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ef75afe-018e-4ac7-b84d-23a62984ee48",
   "metadata": {},
   "source": [
    "#8\n",
    "Performing feature extraction using PCA involves several steps, including standardization, covariance matrix calculation, eigenvalue decomposition, and selecting the number of principal components to retain. Let's walk through these steps for the given dataset with features [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1. **Data Standardization**: Before applying PCA, it's important to standardize the features to have a mean of 0 and a standard deviation of 1. This step ensures that features with different scales do not disproportionately influence the PCA results.\n",
    "\n",
    "2. **Covariance Matrix Calculation**: Calculate the covariance matrix of the standardized dataset. This matrix represents the relationships and correlations between the features.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. This yields the eigenvectors and eigenvalues of the matrix. Eigenvectors represent the directions of maximum variance (principal components), and eigenvalues indicate the variance explained by each eigenvector.\n",
    "\n",
    "4. **Selecting Principal Components**: To decide how many principal components to retain, you can consider the cumulative explained variance or a certain threshold of variance explained. The cumulative explained variance is the fraction of the total variance in the data that is captured by the first k principal components. A common rule of thumb is to retain enough principal components to capture a significant portion of the variance, such as 95% or 99%.\n",
    "\n",
    "The number of principal components you choose to retain depends on the level of information retention you desire. If you aim to retain, for example, 95% of the variance, you would choose the smallest number of principal components that together account for at least 95% of the total variance.\n",
    "\n",
    "Given that the dataset contains 5 features, you will have at most 5 principal components. However, it's likely that not all 5 features contribute equally to the data's variance. Some features might have stronger correlations and larger variances than others.\n",
    "\n",
    "In practice, you would plot the explained variance ratio against the number of principal components and look for an \"elbow point\" in the plot. The elbow point represents the number of principal components where adding more components starts to contribute less significantly to the explained variance. You can then make an informed decision based on this plot.\n",
    "\n",
    "Without actual data and variance values, I can't provide an exact number of principal components to retain. However, as a general guideline, if the first few principal components account for a high percentage of the total variance (e.g., around 90% or higher), you might choose to retain those components. If the cumulative explained variance remains relatively low even with a few components, you might consider retaining more components.\n",
    "\n",
    "Remember that while reducing dimensionality can help with computational efficiency and noise reduction, it's important to strike a balance between retaining enough information for accurate modeling and reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864b759-bb2b-48e1-9f92-d5f1588bcfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
